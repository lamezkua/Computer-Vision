{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true;\n",
       "function code_toggle() {\n",
       "if (code_show){\n",
       "$('div.input').hide();\n",
       "} else {\n",
       "$('div.input').show();\n",
       "}\n",
       "code_show = !code_show\n",
       "}\n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true;\n",
    "function code_toggle() {\n",
    "if (code_show){\n",
    "$('div.input').hide();\n",
    "} else {\n",
    "$('div.input').show();\n",
    "}\n",
    "code_show = !code_show\n",
    "}\n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is /coursedata\n",
      "Data stored in /coursedata/exercise-07-data\n"
     ]
    }
   ],
   "source": [
    "# Description:\n",
    "#   Exercise7 notebook.\n",
    "#\n",
    "# Copyright (C) 2018 Santiago Cortes, Juha Ylioinas\n",
    "#\n",
    "# This software is distributed under the GNU General Public \n",
    "# Licence (version 2 or later); please refer to the file \n",
    "# Licence.txt, included with the software, for details.\n",
    "\n",
    "# Preparations\n",
    "import numpy as np\n",
    "\n",
    "# Select data directory\n",
    "if os.path.isdir('/coursedata'):\n",
    "    course_data_dir = '/coursedata'\n",
    "elif os.path.isdir('../data'):\n",
    "    course_data_dir = '../data'\n",
    "else:\n",
    "    # Specify course_data_dir on your machine\n",
    "    course_data_dir = '/home/jovyan/work/coursedata/'\n",
    "\n",
    "print('The data directory is %s' % course_data_dir)\n",
    "data_dir = os.path.join(course_data_dir, 'exercise-07-data')\n",
    "print('Data stored in %s' % data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-E4850 Computer Vision Exercise Round 7\n",
    "The problems should be solved before the exercise session and solutions returned via\n",
    "MyCourses. <br><br> For this exercise round, upload this notebook(pdf and .ipynb versions) containing your source codes (for Exercise 1) and your answer to the question of Exercise2, and all the answers to the questions of Exercise 3 (VGG practical), see part[1-3].ipynb. Note that it's not necessary to upload part1.ipynb, part2.ipynb or part3.ipynb, because all of the necessary questions related to them are contained in this notebook and you're not expected to do any coding in Exercises 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Comparing  bags-of-words  with  tf-idf  weighting\n",
    "Assume  that  we  have  an  indexed  collection  of  documents  containing  the  five  terms  of the following table where the second row indicates the percentage of documents in which each term appears.<br>\n",
    "\n",
    "| term | cat | dog |mammals | mouse | pet |\n",
    "| --- | :---: | :---: | :---: | :---: | :---: |\n",
    "| **% of documents** | 5 | 20 | 2 | 10 | 60 |\n",
    "\n",
    "Now, given the query $Q=\\{mouse, cat, pet, mammals\\}$, compute the similarity between $Q$ and the following example documents $D1$, $D2$, $D3$, by using the cosine similarity measure and tf-idf weights (i.e. term frequency - inverse document frequency) for the bag-of-words histogram representations of the documents and the query.\n",
    "\n",
    "-  $D1$ = Cat is a pet, dog is a pet, and mouse may be a pet too.\n",
    "-  $D2$ = Cat, dog and mouse are all mammals.\n",
    "-  $D3$ = Cat and dog get along well, but cat may eat a mouse.\n",
    "\n",
    "Ignore other words except the five terms. You may proceed with the following steps:\n",
    "\n",
    "a) Compute and report the inverse document frequency (idf) for each of the five terms. Use the logarithm with base 2. (idf is the logarithm on slide 69 of Lecture 6.)<br>\n",
    "b) Compute the term frequencies for the query and each document. <br>\n",
    "c) Form the tf-idf weighted word occurrence histograms for the query and documents. <br>\n",
    "d) Evaluate the cosine similarity between the query and each document (i.e.\\ normalized scalar product between the weighted occurrence histograms as shown on slide 45).<br> \n",
    "e) Report the relative ranking of the documents. (You should get similarities 0.95, 0.64, and 0.63, but you need to determine which corresponds to which document.)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparing  bags-of-words  with  tf-idf  weighting\n",
    "##--your-code-starts-here--##\n",
    "\n",
    "import math\n",
    "from pprint import pformat\n",
    "from collections import Counter\n",
    "\n",
    "documents = [\"Cat is a pet dog is a pet and mouse may be a pet too\".lower().split(),\n",
    "             \"Cat dog and mouse are all mammals\".lower().split(),\n",
    "             \"Cat and dog get along well, but cat may eat a mouse\".lower().split()]\n",
    "\n",
    "document_histograms = [Counter(d) for d in documents]\n",
    "\n",
    "terms = [\"mouse\", \"cat\", \"pet\", \"mammals\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mouse': 3.3219280948873626,\n",
       " 'cat': 4.321928094887363,\n",
       " 'pet': 0.7369655941662062,\n",
       " 'mammals': 5.643856189774724}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a) Compute and report the inverse document frequency (idf) for each of the five terms. Use the logarithm with base 2.\n",
    "\n",
    "percentage_in_which_term_appears = {\"cat\": 1 / 0.05,\n",
    "                                    \"dog\": 1 / 0.20,\n",
    "                                    \"mammals\": 1 / 0.02,\n",
    "                                    \"mouse\": 1 / 0.10,\n",
    "                                    \"pet\": 1 / 0.6}\n",
    "\n",
    "inverse_document_frequency = { t: math.log(percentage_in_which_term_appears[t], 2) for t in terms }\n",
    "\n",
    "inverse_document_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mouse': [0.06666666666666667, 0.14285714285714285, 0.08333333333333333],\n",
       " 'cat': [0.06666666666666667, 0.14285714285714285, 0.16666666666666666],\n",
       " 'pet': [0.2, 0.0, 0.0],\n",
       " 'mammals': [0.0, 0.14285714285714285, 0.0]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b) Compute the term frequencies for the query and each document.\n",
    "\n",
    "term_frequencies = {t: [h[t] / sum(h.values()) for h in document_histograms] for t in terms}\n",
    "\n",
    "term_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mouse': [0.22146187299249084, 0.47456115641248037, 0.27682734124061353],\n",
       " 'cat': [0.2881285396591575, 0.6174182992696232, 0.7203213491478937],\n",
       " 'pet': [0.14739311883324124, 0.0, 0.0],\n",
       " 'mammals': [0.0, 0.8062651699678177, 0.0]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c) Form the tf-idf weighted word occurrence histograms for the query and documents.\n",
    "\n",
    "tf_idfs = {t: [term_frequencies[t][i] * inverse_document_frequency[t] for i in range(len(documents))] for t in terms}\n",
    "\n",
    "tf_idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mappings: {'cat': 0, 'mammals': 1, 'mouse': 2, 'pet': 3}\n",
      "Cosine similarities: {0: 0.8376508896884339, 1: 0.846729872790561, 2: 0.6460861361314962}\n"
     ]
    }
   ],
   "source": [
    "# d) Evaluate the cosine similarity between the query and each document.\n",
    "\n",
    "def bag2vec(bag, dictionary):\n",
    "    dict_len = len(list(dictionary.values()))\n",
    "    vector = [0 for _ in range(dict_len)]\n",
    "    for term in dictionary.keys():\n",
    "        vector[dictionary[term]] = bag[term]    \n",
    "    return vector\n",
    "\n",
    "def L2_norm(v):\n",
    "    return math.sqrt(sum([x ** 2 for x in v]))\n",
    "\n",
    "def dot(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    return sum([a[i] * b[i] for i in range(len(a))])\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return dot(a, b) / (L2_norm(a) * L2_norm(b))\n",
    "\n",
    "mappings = {t: i for i, t in enumerate(sorted(terms))}\n",
    "\n",
    "print('Mappings: {}'.format(pformat(mappings)))\n",
    "\n",
    "\n",
    "query_vector = bag2vec(Counter([\"mouse\", \"cat\", \"pet\", \"mammals\"]), mappings)\n",
    "document_vectors = [bag2vec({t: tf_idfs[t][i] for t in tf_idfs.keys()}, mappings) for i in range(len(documents))]\n",
    "\n",
    "cosine_similarities = {i: cosine_similarity(dj, query_vector) for i, dj in enumerate(document_vectors)}\n",
    "\n",
    "print('Cosine similarities: {}'.format(pformat(cosine_similarities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Cosine Similarities: [(1, 0.846729872790561), (0, 0.8376508896884339), (2, 0.6460861361314962)]\n",
      "Rankings: ['cat dog and mouse are all mammals',\n",
      " 'cat is a pet dog is a pet and mouse may be a pet too',\n",
      " 'cat and dog get along well, but cat may eat a mouse']\n"
     ]
    }
   ],
   "source": [
    "# e) Report the relative ranking of the documents.\n",
    "\n",
    "ranked_cosine_similarities = list(reversed(sorted(cosine_similarities.items(), key=lambda x: x[1])))\n",
    "rankings = [\" \".join(documents[i]) for i, _ in ranked_cosine_similarities]\n",
    "\n",
    "print('Ranked Cosine Similarities: {}'.format(pformat(ranked_cosine_similarities)))\n",
    "print('Rankings: {}'.format(pformat(rankings)))\n",
    "\n",
    "##--your-code-ends-here--##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Precision  and  recall\n",
    "There is a database of 10000 images and a user, who is only interested in images which contain a car. It is known that there are 500 such images in the database. An  automatic image retrieval system retrieves 300 car images and 50 other images from the database. Determine and report the precision and recall of the retrieval  system in this particularcase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type your answer here:\n",
    "\n",
    "Precision and recall are determined as follows:\n",
    "\n",
    "$$\n",
    "  \\text{precision} = \\frac{\\text{relevant images}}{\\text{returned images}}\\\\\n",
    "  \\space\\\\\n",
    "  \\text{recall} = \\frac{\\text{relevant images}}{\\text{total relevant images}}\n",
    "$$\n",
    "\n",
    "According to the given information, we can report the following:\n",
    "\n",
    "$$\n",
    "  \\text{precision} = \\frac{300}{350}\\\\\n",
    "  \\space\\\\\n",
    "  \\text{recall} = \\frac{300}{500}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - VGG practical on object instance recognition\n",
    "See the questions in part[1-3].ipynb and write your answers here.\n",
    "\n",
    "Part1:\n",
    "Stage I.A (two questions)\n",
    "Stage I.B (two questions)\n",
    "Stage I.C (one question)\n",
    "\n",
    "Part2 (one question)\n",
    "\n",
    "Part3:\n",
    "Stage III.A (three questions)\n",
    "Stage III.B (one question)\n",
    "Stage III.C (two questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type your answers here:\n",
    "\n",
    "# Part I: Sparse features for matching object instances\n",
    "## Stage I.A: SIFT features detector\n",
    "\n",
    "<b>Question</b>: Note the change in density of detections across the image. Why does it change? Will it be a problem for matching? How could it be avoided?\n",
    "\n",
    "Change in density of detections was probably caused by the variance in pixel intensity. For example, in the right image we have a large region with shadows.\n",
    "\n",
    "This variance probably generates problems for matching: the left image will have more detected features than the right image.\n",
    "\n",
    "This problem might be solved by lowering the peakThreshold value, which allows lower-contrast features to be detected. However this might result in more noise.\n",
    "\n",
    "\n",
    "<b>Question</b>: Occasionally, a feature is detected multiple times, with different orientations. This may happen when the orientation assignment is ambiguous. Which kind of image structure would result in ambiguous orientation assignment?\n",
    "\n",
    "An image structure with an even intensity radial gradient arc, which could have equal intensity along several orientation bins, causing rotational ambiguity (ambiguous orientation assignment).\n",
    "\n",
    "\n",
    "## Stage I.B: SIFT features detectors and matching between images\n",
    "\n",
    "<b>Question</b>: Note the descriptors are computed over a much larger region (shown in blue) than the detection (shown in green). Why?\n",
    "\n",
    "This happens probably because the keypoints only start to become unique at a sufficient scale and descriptor area.\n",
    "\n",
    "\n",
    "<b>Question</b>: Notice that there are many mismatches. Examine some of the mismatches to understand why the mistakes are being made. For example, is the change in lighting a problem? What additional constraints can be applied to remove the mismatches?\n",
    "\n",
    "The mismatches might be caused because there are a lot of small features in both images for which there is a large amount of ambiguity in relation to the features in the corresponding image that are actually the correct match.\n",
    "\n",
    "This problem could be reduced by searching for the nearest neighbour to the matched point and comparing similarity: rejecting the matched feature as a match candidate if the nearest neighbour distance is below some threshold.\n",
    "\n",
    "\n",
    "## Stage I.C: Improving SIFT matching using Lowe’s second nearest neighbour test\n",
    "\n",
    "<b>Question</b>: Examine some of the remaining mismatches to understand why they have occurred. How could they be removed?\n",
    "\n",
    "The remaining mismatches could be removed with some sort of geometric transform fitting technique or RANSAC: we look at all the gradients of the matched feature lines and keep all the inliers, rejecting the outlier gradients which are clearly mismatches.\n",
    "\n",
    "\n",
    "\n",
    "## Part II: Affine co-variant detectors\n",
    "\n",
    "<b>Question</b>: The transformation between the images induced by the plane is a planar homography. The detections are only affine co-variant (not as general as a planar homography). So how can descriptors computed on these detections possibly match?\n",
    "\n",
    "SIFT matching works on local image regions, which are small enough such that the projective transformation can be approximated by an affine transformation. Then, since SIFT is affine-invariant, we can exploit this fact continuing to do the  matching as long as we can approximate the projective transformation with an affine one.\n",
    "\n",
    "\n",
    "\n",
    "## Part III: Towards large scale retrieval\n",
    "### Stage III.A: Accelerating descriptor matching with visual words\n",
    "\n",
    "<b>Questions</b>:\n",
    "- The size of the vocabulary (the number of clusters) is an important parameter in visual word algorithms. How does the size affect the number of inliers and the difficulty of computing the transformation?\n",
    "\n",
    "As we increase the size of the vocabulary, we incrase the number of potential matches, as there are more evaluated points that we could potentially match with.\n",
    "\n",
    "This means that we need to run RANSAC for a longer number of iterations - more specifically we need to run it for $N = \\frac{\\log(1 - p)}{\\log(1 - (1 - e)^s)}$ iterations in order to find enough inliers for a given transformation.\n",
    "\n",
    "As we detect more and more points, the probability that the points are inliers goes down logarithmically. This means that we need to run RANSAC for a larger number of iterations thus the difficulty of computing the transformation increases.\n",
    "\n",
    "\n",
    "- In the above procedure the time required to convert the descriptors into visual words was not accounted for. Why?\n",
    "\n",
    "This time is not accounted because the descriptors are saved into a database and then they are matched either to feature descriptors in the image directly or to the nearest neighbour to those features, which can be looked up in $O(\\log(d))$ time on the existing hierarchical tree.\n",
    "\n",
    "\n",
    "- What is the speedup in searching a large, fixed database of 10, 100, 1000 images?\n",
    "\n",
    "Taking in account that we are using a tree structure, we can search the inverted index mapping features to images in the tree. We can find the nearest neighbour in logarithmic time and then map to the the relevant database images.\n",
    "\n",
    "The speedup depends on the number of words in the tree: fewer words means that more database images are going to end up in a bin for a given word and this means that more images need to be searched and aligned. We would expect a 2x, 3x and 4x improvement in performance respectively as the complexity of the k-d tree increases logarithmically with the number of leaf nodes (as opposed to linearly).\n",
    "\n",
    "\n",
    "### Step III.b: Searching with an inverted index\n",
    "\n",
    "Task: How many erroneously matched images do you count in the top results?\n",
    "\n",
    "Around 15.\n",
    "\n",
    "\n",
    "<b>Question</b>: Why does the top image have a score of 1 (0.9698... see the NOTE above)?\n",
    "\n",
    "The top image has this score because it matches the query image exactly: it contains all of the same highly-relevant features of the query image.\n",
    "\n",
    "\n",
    "### Stage III.C: Geometric rescoring\n",
    "\n",
    "<b>Question</b>: Why is the top score much larger than 1 now?<br>\n",
    "\n",
    "This top score is now given by the number of inliers to the transformation as opposed to the term-frequency inverse-document-frequency measure: the number of inliers is not a metric but a count. If we could detect more features in an image and it had more inliers, then it is more likely to be a highly relevant or matching image.\n",
    "\n",
    "<b>Question</b>: Are the retrieval results improved after geometric verification?\n",
    "\n",
    "Not completely: we still have 14 incorrect matches which were similar in nature to the query but were not an image of the same thing. However, all of the top-ranked matches are now of the same landmark and have substantially different scores to those which were matched purely on features. This shows that the results have improved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
